{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.106:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 구조\n",
    "\n",
    "spark는 RDD, Dataframe, Dataset 세가지 데이터 구조 제공\n",
    "\n",
    "- RDD: 데이터가 **비구조적**인 경우 사용하기 적합, 모델 schema를 정하지 않고 사용 가능.\n",
    "- Dataframe: 데이터가 schema와 datatype을 가진 **구조적**인 경우 사용.\n",
    "- Spark의 RDD, Dataframe 모두 immutable이기 때문에 일단 생성되고 나면 원본 수정 불가.\n",
    "\n",
    "| 데이터 구조 | 설명 |\n",
    "|:--------|:--------|\n",
    "| **RDD** | 비구조적, schema free, low-level |\n",
    "| **Dataframe** | 구조적, schema 가짐 |  \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RDD\n",
    "- RDD\n",
    "    - Resilient : 어느 한 노드에서 작업이 실패하면 다른 노드에서 실행\n",
    "    - Distributed : 클러스터로 구성된 여러 노드에 분산해서 처리\n",
    "    - Dataset : 데이터 구조\n",
    "- 데이터를 저장하고 있는 Data Set\n",
    "- 여러 컴퓨터에 분산해서 사용할 수 있다는 점이 특징\n",
    "- RDD는 데이터가 비구조적인 경우 사용하기 적합\n",
    "- 모델 schema를 정하지 않고 사용 가능\n",
    "- RDD는 Python list, 파일, hdfs 등 다양한 자료에서 생성할 수 있고, 생성된 자료는 수정할 수 없는 read-only(immutable이기 때문)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 RDD 생성하기\n",
    "RDD는 sparkContext로부터 생성됨\n",
    "\n",
    "| 생성 방법 | 설명 | 함수 |\n",
    "|:--------|:--------|:--------|\n",
    "| **내부에서 읽기** | Python list에서 생성 | `parallelize()` |\n",
    "| **외부에서 읽기** | 파일, HDFS, HBase ... | `textFile(\"data/*txt\")` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 list에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.first() # 첫번째 요소 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.count() # 요소 갯수 돌려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(3) # 앞에 요소 3개 list로 돌려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.collect() # 모든 요소 list로 돌려줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 파일에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/file2RDD.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/file2RDD.txt\n",
    "My name is Parkapark.\n",
    "This is an jupyter notebook about Apache Spark.\n",
    "Apahce Spark is an open source cluster computing framework.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "Written by Parkapark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD1 = spark.sparkContext.textFile(os.path.join(\"data/file2RDD.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Parkapark.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Parkapark.',\n",
       " 'This is an jupyter notebook about Apache Spark.',\n",
       " 'Apahce Spark is an open source cluster computing framework.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Parkapark.',\n",
       " 'This is an jupyter notebook about Apache Spark.',\n",
       " 'Apahce Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'Written by Parkapark.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 CSV에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/CSV2RDD.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/CSV2RDD.csv\n",
    "0, 0\n",
    "1, 1\n",
    "2, 2\n",
    "3, 3\n",
    "4, 4\n",
    "5, 5\n",
    "6, 6\n",
    "7, 7\n",
    "8, 8\n",
    "9, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0\r\n",
      "1, 1\r\n",
      "2, 2\r\n",
      "3, 3\r\n",
      "4, 4\r\n",
      "5, 5\r\n",
      "6, 6\r\n",
      "7, 7\r\n",
      "8, 8\r\n",
      "9, 9\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/CSV2RDD.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD2 = spark.sparkContext.textFile(os.path.join(\"data/CSV2RDD.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0, 0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0, 0', '1, 1', '2, 2']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0, 0',\n",
       " '1, 1',\n",
       " '2, 2',\n",
       " '3, 3',\n",
       " '4, 4',\n",
       " '5, 5',\n",
       " '6, 6',\n",
       " '7, 7',\n",
       " '8, 8',\n",
       " '9, 9']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD3 = myRDD2.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', ' 0'],\n",
       " ['1', ' 1'],\n",
       " ['2', ' 2'],\n",
       " ['3', ' 3'],\n",
       " ['4', ' 4'],\n",
       " ['5', ' 5'],\n",
       " ['6', ' 6'],\n",
       " ['7', ' 7'],\n",
       " ['8', ' 8'],\n",
       " ['9', ' 9']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 RDD API\n",
    "`데이터 변환` - transformation  \n",
    "`연산` - action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 데이터 변환\n",
    "| 함수 | 설명 | 예제 |\n",
    "|:--------|:--------|:--------|\n",
    "| map(x) | 요소별로 x를 적용해서 결과 RDD 돌려줌 | .map(lambda x:x.split(',')) |\n",
    "| filter(x) | 요소별로 선별하여 x를 적용해서 결과 RDD 돌려줌 | .filter(lambda x: \"Spark\" in x) |\n",
    "| flatMap(x) | 요소별로 x를 적용하고, flat해서 결과 RDD 돌려줌 | .flatMap(lambda x: x.split(',')) |\n",
    "| groupByKey() | key를 그룹해서 iterator를 돌려줌 |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.1 map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['m'], ['y'], [], ['n'], ['a'], ['m'], ['e'], [], ['i'], ['s'], [], ['p'], ['a'], ['r'], ['k']]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"my name is park\"\n",
    "res = list(map(lambda x:x.split(),sentence))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'name', 'is', 'park'], ['Here', 'is', 'PublicAI']]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"my name is park\",\"Here is PublicAI\"]\n",
    "res = list(map(lambda x:x.split(),sentence))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.2 filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55, 89]\n"
     ]
    }
   ],
   "source": [
    "fibo = [0,1,1,2,3,5,8,13,21,34,55,89]\n",
    "res = list(filter(lambda x: x % 2, fibo))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Parkapark.',\n",
       " 'This is an jupyter notebook about Apache Spark.',\n",
       " 'Apahce Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'Written by Parkapark.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines having 'Parkapark':  2\n"
     ]
    }
   ],
   "source": [
    "myRDD_parkapark = myRDD1.filter(lambda line: \"Parkapark\" in line)\n",
    "print(\"Number of lines having 'Parkapark': \",myRDD_parkapark.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.3 flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Parkapark.',\n",
       " 'This is an jupyter notebook about Apache Spark.',\n",
       " 'Apahce Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'Written by Parkapark.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_flatmap = myRDD1.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Parkapark.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'jupyter',\n",
       " 'notebook',\n",
       " 'about',\n",
       " 'Apache',\n",
       " 'Spark.',\n",
       " 'Apahce',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'framework.',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Written',\n",
       " 'by',\n",
       " 'Parkapark.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.4 groupByKey()\n",
    "- 자세히는 더 나중에 pairRDD에서 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Parkapark.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'jupyter',\n",
       " 'notebook',\n",
       " 'about',\n",
       " 'Apache',\n",
       " 'Spark.',\n",
       " 'Apahce',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'framework.',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Written',\n",
       " 'by',\n",
       " 'Parkapark.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', <pyspark.resultiterable.ResultIterable at 0x124578280>),\n",
       " ('is', <pyspark.resultiterable.ResultIterable at 0x1236b0760>),\n",
       " ('Parkapark.', <pyspark.resultiterable.ResultIterable at 0x124586940>),\n",
       " ('an', <pyspark.resultiterable.ResultIterable at 0x123a2c850>),\n",
       " ('jupyter', <pyspark.resultiterable.ResultIterable at 0x123cb9730>),\n",
       " ('Apache', <pyspark.resultiterable.ResultIterable at 0x123cb9640>),\n",
       " ('Spark.', <pyspark.resultiterable.ResultIterable at 0x124566c10>),\n",
       " ('Spark', <pyspark.resultiterable.ResultIterable at 0x124566e80>),\n",
       " ('open', <pyspark.resultiterable.ResultIterable at 0x124566d90>),\n",
       " ('source', <pyspark.resultiterable.ResultIterable at 0x124566220>),\n",
       " ('My', <pyspark.resultiterable.ResultIterable at 0x124566700>),\n",
       " ('This', <pyspark.resultiterable.ResultIterable at 0x124566460>),\n",
       " ('notebook', <pyspark.resultiterable.ResultIterable at 0x124566eb0>),\n",
       " ('about', <pyspark.resultiterable.ResultIterable at 0x1245667c0>),\n",
       " ('Apahce', <pyspark.resultiterable.ResultIterable at 0x124566f70>),\n",
       " ('cluster', <pyspark.resultiterable.ResultIterable at 0x124566ee0>),\n",
       " ('computing', <pyspark.resultiterable.ResultIterable at 0x124566340>),\n",
       " ('framework.', <pyspark.resultiterable.ResultIterable at 0x124566be0>),\n",
       " ('Written', <pyspark.resultiterable.ResultIterable at 0x1245664f0>),\n",
       " ('by', <pyspark.resultiterable.ResultIterable at 0x124566730>)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_flatmap\\\n",
    ".map(lambda x:(x,1))\\\n",
    ".groupByKey()\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.5 flatMap VS map  \n",
    "- flatMap() \n",
    "    - 리스트 안에 또 리스트가 있는 경우 이를 하나의 리스트로 만듬\n",
    "    - 모든 단어를 하나의 리스트로 만듬\n",
    "- map()\n",
    "    - 리스트 안에 또 리스트가 있는 구조를 보존하고 처리함  \n",
    "    - 파일의 줄마다 리스트를 만듬\n",
    "| 줄 | 원본 | flatMap() 결과 | map() 결과 |\n",
    "|:--------|:--------|:--------|:--------|\n",
    "| 1 | My name is | My name is | ['My','name','is'] |\n",
    "| 2 | This is a jupyter | This is a jupyter | ['This','is','a','jupyter'] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/file2RDD.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/file2RDD.txt\n",
    "My name is Parkapark.\n",
    "This is an jupyter notebook about Apache Spark.\n",
    "Apahce Spark is an open source cluster computing framework.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "Written by Parkapark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD1 = spark.sparkContext.textFile(os.path.join(\"data/file2RDD.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD1_flatmap = myRDD1.flatMap(lambda x:x.split(\" \")).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Parkapark.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'jupyter',\n",
       " 'notebook',\n",
       " 'about']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1_flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Parkapark. This is an jupyter notebook about "
     ]
    }
   ],
   "source": [
    "for i in myRDD1_flatmap:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD1_map = myRDD1.map(lambda x:x.split(\" \")).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', 'name', 'is', 'Parkapark.'],\n",
       " ['This', 'is', 'an', 'jupyter', 'notebook', 'about', 'Apache', 'Spark.']]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Parkapark.'] ['This', 'is', 'an', 'jupyter', 'notebook', 'about', 'Apache', 'Spark.'] "
     ]
    }
   ],
   "source": [
    "for i in myRDD1_map:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 연산 action\n",
    "| 함수 | 설명 | 예제 |\n",
    "|:--------|:--------|:--------|\n",
    "| reduce(fn) | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 | .reduce(lambda x,y:x+y) |\n",
    "| collect() | 모든 요소를 결과 list로 돌려줌 |  |\n",
    "| count() | 요소 갯수를 결과 list로 돌려줌 |  |\n",
    "| take(n) | collect()는 전체지만, n개만 돌려줌 | take(3) |\n",
    "| countByKey() | key별 갯수를 세는 함수 | countByKey().items() |\n",
    "| foreach(fn) | 각 데이터 항목에 함수 fn을 적용 |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.1 reduce()\n",
    "reduce()는 lambda 함수를 사용해서 입력 데이터를 하나씩 서로 더해서 x+y 결과로 만들어 짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_reduce = spark.sparkContext.parallelize(range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_reduce.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_reduce.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.2 countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Parkapark.',\n",
       " 'This is an jupyter notebook about Apache Spark.',\n",
       " 'Apahce Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'Written by Parkapark.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_flatmap = myRDD1.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "myRDD_countByKey = myRDD_flatmap\\\n",
    ".map(lambda x:(x,1))\\\n",
    ".countByKey() # 결과값 dictionary로 출력\n",
    "\n",
    "print(type(myRDD_countByKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'My': 1,\n",
       "             'name': 1,\n",
       "             'is': 3,\n",
       "             'Parkapark.': 2,\n",
       "             'This': 1,\n",
       "             'an': 2,\n",
       "             'jupyter': 1,\n",
       "             'notebook': 1,\n",
       "             'about': 1,\n",
       "             'Apache': 5,\n",
       "             'Spark.': 1,\n",
       "             'Apahce': 1,\n",
       "             'Spark': 5,\n",
       "             'open': 1,\n",
       "             'source': 1,\n",
       "             'cluster': 1,\n",
       "             'computing': 1,\n",
       "             'framework.': 1,\n",
       "             'Written': 1,\n",
       "             'by': 1})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_items'>\n"
     ]
    }
   ],
   "source": [
    "myRDD_countByKey = myRDD_flatmap\\\n",
    ".map(lambda x:(x,1))\\\n",
    ".countByKey()\\\n",
    ".items() # .items() 붙이면 리스트로 변환 가능\n",
    "\n",
    "print(type(myRDD_countByKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('My', 1), ('name', 1), ('is', 3), ('Parkapark.', 2), ('This', 1), ('an', 2), ('jupyter', 1), ('notebook', 1), ('about', 1), ('Apache', 5), ('Spark.', 1), ('Apahce', 1), ('Spark', 5), ('open', 1), ('source', 1), ('cluster', 1), ('computing', 1), ('framework.', 1), ('Written', 1), ('by', 1)])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.3 foreach()\n",
    "`foreach()`는 action이지만, 다른 action들과 다르게 반환 값이 없음\n",
    "각 요소에 적용하는 역할에 대해 유사한 `map()`이 있으나 `map()`은 요소에 대해 계산을 하고, 그 값을 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"570\" alt=\"foreach\" src=\"https://user-images.githubusercontent.com/71860179/98218847-9a090980-1f8f-11eb-82df-d770b2486308.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pair RDD\n",
    "- key,value 쌍으로 구성된 RDD\n",
    "- key에 대해 연산을 하는 byKey() 또는 value에 대해  연산을 하는 byValue() 함수 사용 가능  \n",
    "\n",
    "\n",
    "------\n",
    "| 구분 | 예제 |\n",
    "|:--------|:--------|\n",
    "| groupByKey() | 같은 key를 grouping, 부분 partition에 먼저 reduce하지 않고, 전체로 계산한다. |\n",
    "| reduceByKey() | 같은 key의 value를 합계, 부분 partition에서 먼저 reduce하고, 전체로 계산하다. grouping + aggregation. 즉 reduceByKey = groupByKey().reduce() |\n",
    "| mapValues() | PairRDD는 key,value가 있기 마련이다. value에 대해 적용하는 함수이다. 즉 key가 아니라 value에 적용하는 함수이다. |\n",
    "| combineByKey() | 키별로 합계, 개수 (key,(sum,count))를 계산 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 pair RDD 생성\n",
    "\n",
    "partition 1,2,3로 데이터가 분할 되어 있다 가정\n",
    "\n",
    "| P1 | P2 | P3 |\n",
    "|:--------|:--------|:--------|\n",
    "| (key1,1) | (key1,1) | (key1,1) |\n",
    "| (key1,1) | (key2,1) | (key1,1) |\n",
    "| (key1,1) |  | (key2,1) |\n",
    "| (key2,1) |  | (key2,1) |\n",
    "| (key2,1) |  |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRDD = spark.sparkContext.parallelize(testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 1),\n",
       " ('key1', 1),\n",
       " ('key1', 1),\n",
       " ('key2', 1),\n",
       " ('key2', 1),\n",
       " ('key1', 1),\n",
       " ('key2', 1),\n",
       " ('key1', 1),\n",
       " ('key1', 1),\n",
       " ('key2', 1),\n",
       " ('key2', 1)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key1',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 pair RDD - groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', <pyspark.resultiterable.ResultIterable at 0x124f3ce20>),\n",
       " ('key2', <pyspark.resultiterable.ResultIterable at 0x124efdaf0>)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 pair RDD - reduceByKey()\n",
    "reduceByKey() == groupByKey().reduce(): 실제로 되지는 않고 의미상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 6), ('key2', 5)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 pair RDD - mapValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 pair RDD - combineByKey()\n",
    "key 별로 `(key, (sum, count))`를 계산 함  \n",
    "\n",
    "| 구분 | combiner | merge values | merge combiner |\n",
    "|:--------|:--------|:--------|:--------|\n",
    "| **설명** | 각 키에 대해 **(value,1)** 튜플 생성 | 값을 더해 나감 (sum,count), `sum+value`, `count+1` | partition 별로 conbiner를 더함 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 stopwords(불용어) 제거하고 word count 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.0 데이터 쓰기, 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/spark_wiki.txt\n",
    "Apache Spark is and open-source distributed general-purpose cluster-computing framework.\n",
    "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "Originally developed at the University of California, Berkeley's AMP Lab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n",
    "Apache Spark has its architectural foundation in the resilient distributed dataset(RDD), a read only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.\n",
    "The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.\n",
    "In Spark 1.x, the RDD was the primary application programming interface(API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD  API is not deprecated.\n",
    "The RDD technology still underlies the Dataset API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_word_count = spark.sparkContext\\\n",
    ".textFile(os.path.join(\"data/spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is and open-source distributed general-purpose cluster-computing framework.',\n",
       " 'Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.',\n",
       " \"Originally developed at the University of California, Berkeley's AMP Lab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_word_count.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 stopwords 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/site-packages (from nltk) (2020.10.28)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = myRDD_word_count.flatMap(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'and',\n",
       " 'open-source',\n",
       " 'distributed',\n",
       " 'general-purpose',\n",
       " 'cluster-computing',\n",
       " 'framework.',\n",
       " 'Spark']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 filter() \n",
    "- stopwords 리스트에 있으면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count1 = word_count.filter(lambda x: x.lower() not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'open-source',\n",
       " 'distributed',\n",
       " 'general-purpose',\n",
       " 'cluster-computing',\n",
       " 'framework.',\n",
       " 'Spark',\n",
       " 'provides',\n",
       " 'interface']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 map()\n",
    "- 단어별로 (x,1)로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count2 = word_count1.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 1),\n",
       " ('Spark', 1),\n",
       " ('open-source', 1),\n",
       " ('distributed', 1),\n",
       " ('general-purpose', 1),\n",
       " ('cluster-computing', 1),\n",
       " ('framework.', 1),\n",
       " ('Spark', 1),\n",
       " ('provides', 1),\n",
       " ('interface', 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 reduceByKey()\n",
    "- 동일한 단어의 value, 즉 갯수를 서로 합함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count3 = word_count2.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 3),\n",
       " ('Spark', 6),\n",
       " ('open-source', 1),\n",
       " ('general-purpose', 1),\n",
       " ('cluster-computing', 1),\n",
       " ('provides', 1),\n",
       " ('programming', 2),\n",
       " ('entire', 1),\n",
       " ('clusters', 1),\n",
       " ('implicit', 1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 묶어서 하나로 많이 나온 단어 순으로 출력\n",
    "- map(lambda x:(x[1],x[0])) : 내림차순 정렬하기 위해서 단어의 숫자 앞으로 오도록 x[1] 과 x[0] 바꾸기\n",
    "- sortByKey(False) : 내림차순 정렬, default가 오름차순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_final = (\n",
    "    myRDD_word_count\n",
    "    .flatMap(lambda x:x.split(' '))\n",
    "    .filter(lambda x:x.lower() not in stop_words)\n",
    "    .map(lambda x:(x,1))\n",
    "    .reduceByKey(lambda x,y:x+y)\n",
    "    .map(lambda x:(x[1],x[0]))\n",
    "    .sortByKey(False) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 'Spark'),\n",
       " (3, 'Apache'),\n",
       " (3, 'distributed'),\n",
       " (3, 'API'),\n",
       " (3, 'Dataset'),\n",
       " (3, 'RDD'),\n",
       " (2, 'programming'),\n",
       " (2, 'data'),\n",
       " (2, 'maintained'),\n",
       " (2, 'API.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_final.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

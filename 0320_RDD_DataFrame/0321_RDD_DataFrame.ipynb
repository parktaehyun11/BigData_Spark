{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://218.38.137.27:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 성적 합계 및 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 이름 | 과목 | 점수 |\n",
    "|:--------|:--------|:--------|\n",
    "| 홍길동 | Korean | 85 |\n",
    "| 홍길동 | English | 80 |\n",
    "| 임꺽정 | Korean | 60.5 |\n",
    "| 임꺽정 | English | 80.5 |\n",
    "| 세종대왕 | Korean | 100 |\n",
    "| 세종대왕 | English | 90 |\n",
    "\n",
    "- 문제1: 이름으로 합계를 구해보자. ex) '홍길동' 165.0, '임꺽정' 141.0, '세종대왕' 190\n",
    "- 문제2: 과목으로 합계를 게산해보자. ex) 'Korean' 245.5,'English' 250.5\n",
    "- 문제3: 이름으로 합계와 개수를 구해보자. ex) '홍길동' (165.0, 2), '임꺽정' (141.0, 2), '세종대왕' (190.0, 2)\n",
    "- 문제4: 이름으로 평균을 계산해보자. ex) '홍길동' 82.5, '임꺽정' 70.5, '세종대왕' 95.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"'홍길동','Korean', 85\",\n",
    "    \"'홍길동','English', 80\",\n",
    "    \"'임꺽정','Korean', 60.5\",\n",
    "    \"'임꺽정','English', 80.5\",\n",
    "    \"'세종대왕','Korean', 100\",\n",
    "    \"'세종대왕','English', 90\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제1: 이름으로 합계를 구해보자. ex) '홍길동' 165.0, '임꺽정' 141.0, '세종대왕' 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks = spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'홍길동','Korean', 85\",\n",
       " \"'홍길동','English', 80\",\n",
       " \"'임꺽정','Korean', 60.5\",\n",
       " \"'임꺽정','English', 80.5\",\n",
       " \"'세종대왕','Korean', 100\",\n",
       " \"'세종대왕','English', 90\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q1 = myRDD_marks.map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'홍길동'\", \"'Korean'\", ' 85'],\n",
       " [\"'홍길동'\", \"'English'\", ' 80'],\n",
       " [\"'임꺽정'\", \"'Korean'\", ' 60.5'],\n",
       " [\"'임꺽정'\", \"'English'\", ' 80.5'],\n",
       " [\"'세종대왕'\", \"'Korean'\", ' 100'],\n",
       " [\"'세종대왕'\", \"'English'\", ' 90']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q1 = myRDD_marks\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0],float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'홍길동'\", 85.0),\n",
       " (\"'홍길동'\", 80.0),\n",
       " (\"'임꺽정'\", 60.5),\n",
       " (\"'임꺽정'\", 80.5),\n",
       " (\"'세종대왕'\", 100.0),\n",
       " (\"'세종대왕'\", 90.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q1 = myRDD_marks\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'홍길동'\", 165.0), (\"'세종대왕'\", 190.0), (\"'임꺽정'\", 141.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'홍길동' 165.0\n",
      "'세종대왕' 190.0\n",
      "'임꺽정' 141.0\n"
     ]
    }
   ],
   "source": [
    "res = myRDD_marks_Q1.collect()\n",
    "for i in res:\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제2: 과목으로 합계를 게산해보자. ex) 'Korean' 245.5,'English' 250.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "myRDD_marks = spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'홍길동','Korean', 85\",\n",
       " \"'홍길동','English', 80\",\n",
       " \"'임꺽정','Korean', 60.5\",\n",
       " \"'임꺽정','English', 80.5\",\n",
       " \"'세종대왕','Korean', 100\",\n",
       " \"'세종대왕','English', 90\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q2 = myRDD_marks.map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'홍길동'\", \"'Korean'\", ' 85'],\n",
       " [\"'홍길동'\", \"'English'\", ' 80'],\n",
       " [\"'임꺽정'\", \"'Korean'\", ' 60.5'],\n",
       " [\"'임꺽정'\", \"'English'\", ' 80.5'],\n",
       " [\"'세종대왕'\", \"'Korean'\", ' 100'],\n",
       " [\"'세종대왕'\", \"'English'\", ' 90']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q2 = myRDD_marks\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[1],float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'Korean'\", 85.0),\n",
       " (\"'English'\", 80.0),\n",
       " (\"'Korean'\", 60.5),\n",
       " (\"'English'\", 80.5),\n",
       " (\"'Korean'\", 100.0),\n",
       " (\"'English'\", 90.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q2 = myRDD_marks\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[1],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'Korean'\", 245.5), (\"'English'\", 250.5)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Korean' 245.5\n",
      "'English' 250.5\n"
     ]
    }
   ],
   "source": [
    "res = myRDD_marks_Q2.collect()\n",
    "for i in res:\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제3: 이름으로 합계와 개수를 구해보자. ex) '홍길동' (165.0, 2), '임꺽정' (141.0, 2), '세종대왕' (190.0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks = spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'홍길동','Korean', 85\",\n",
       " \"'홍길동','English', 80\",\n",
       " \"'임꺽정','Korean', 60.5\",\n",
       " \"'임꺽정','English', 80.5\",\n",
       " \"'세종대왕','Korean', 100\",\n",
       " \"'세종대왕','English', 90\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q3 = myRDD_marks.map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'홍길동'\", \"'Korean'\", ' 85'],\n",
       " [\"'홍길동'\", \"'English'\", ' 80'],\n",
       " [\"'임꺽정'\", \"'Korean'\", ' 60.5'],\n",
       " [\"'임꺽정'\", \"'English'\", ' 80.5'],\n",
       " [\"'세종대왕'\", \"'Korean'\", ' 100'],\n",
       " [\"'세종대왕'\", \"'English'\", ' 90']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q3 = myRDD_marks\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0],float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'홍길동'\", 85.0),\n",
       " (\"'홍길동'\", 80.0),\n",
       " (\"'임꺽정'\", 60.5),\n",
       " (\"'임꺽정'\", 80.5),\n",
       " (\"'세종대왕'\", 100.0),\n",
       " (\"'세종대왕'\", 90.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q3 = myRDD_marks\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0],float(x[2])))\\\n",
    "    .combineByKey(\n",
    "    (lambda x: (x,1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value : (acc[0]+value,acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'홍길동'\", (165.0, 2)), (\"'세종대왕'\", (190.0, 2)), (\"'임꺽정'\", (141.0, 2))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제4: 이름으로 평균을 계산해보자. ex) '홍길동' 82.5, '임꺽정' 70.5, '세종대왕' 95.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_marks_Q4 = myRDD_marks_Q3\\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'홍길동'\", 82.5), (\"'세종대왕'\", 95.0), (\"'임꺽정'\", 70.5)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD_marks_Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RDD 통해 word vector 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/spark_wiki.txt\n",
    "Apache Spark is and open-source distributed general-purpose cluster-computing framework.\n",
    "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "Originally developed at the University of California, Berkeley's AMP Lab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n",
    "Apache Spark has its architectural foundation in the resilient distributed dataset(RDD), a read only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.\n",
    "The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.\n",
    "In Spark 1.x, the RDD was the primary application programming interface(API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD  API is not deprecated.\n",
    "The RDD technology still underlies the Dataset API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD_word_count = spark.sparkContext.textFile(os.path.join(\"data/spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 단어의 리스트 생성 - flatMap() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myWords = myRDD_word_count.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어 갯수:  138\n"
     ]
    }
   ],
   "source": [
    "print(\"총 단어 갯수: \",myWords.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myWords1 = myWords.filter(lambda x: x.lower() not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어 갯수:  84\n"
     ]
    }
   ],
   "source": [
    "print(\"총 단어 갯수: \",myWords1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 단어 tuple 생성\n",
    "- 불필요 구문(쉼표, 마침표, ...)\n",
    "- (단어, 1) 구조 만들어서 나중에 연산 가능하게 만들어 놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myWords2 = (myWords1\n",
    "            .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.x', 1), ('2.x', 1), ('abstraction', 1), ('amp', 1), ('apache', 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWords2.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 빈도 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "myWords_reduceByKey = myWords2.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.x', 1),\n",
       " ('2.x', 1),\n",
       " ('abstraction', 1),\n",
       " ('amp', 1),\n",
       " ('apache', 3),\n",
       " ('api', 5),\n",
       " ('application', 1),\n",
       " ('architectural', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWords_reduceByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 groupByKey(), mapValues()\n",
    "- groupByKey() 통해 집단화 되어서 PariRDD가 되어 key-value 쌍으로 구성됨\n",
    "- mapValues()를 통해 각 쌍의 value에 대해 sum 연산을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "myWords_groupByKey1 = myWords2.groupByKey().mapValues(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.x', 1),\n",
       " ('2.x', 1),\n",
       " ('abstraction', 1),\n",
       " ('amp', 1),\n",
       " ('apache', 3),\n",
       " ('api', 5),\n",
       " ('application', 1),\n",
       " ('architectural', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWords_groupByKey1.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 groupByKey(), map, len\n",
    "- mapValues() 대신 map()과 len()을 통해 동일한 결과 얻어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "myWords_groupByKey2 = myWords2.groupByKey().map(lambda x: (x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.x', 1),\n",
       " ('2.x', 1),\n",
       " ('abstraction', 1),\n",
       " ('amp', 1),\n",
       " ('apache', 3),\n",
       " ('api', 5),\n",
       " ('application', 1),\n",
       " ('architectural', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWords_groupByKey2.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DataFrame\n",
    "- 행,열로 구조화도니 데이터 구조, RDD인데 schema 얹어 놓은 것\n",
    "- spark 1.0 당시 schemaRDD라는 명칭으로 제공 됨\n",
    "- 대용량 데이터를 처리하기 위해 만들어진 프레임워크로 분산해서 사용할 수 있게 고안 됨\n",
    "- 모델 schema를 설정해서 사용함\n",
    "- '열(row)'에 대해 명칭 및 '데이터 type'을 갖고 있고, 이를 지켜서 데이터 저장함\n",
    "\n",
    "**Schema**\n",
    "- row는 dataframe의 행\n",
    "- python의 list, dict를 사용해서 row 구성할 수 있음\n",
    "- column은 dataframe의 열\n",
    "\n",
    "\n",
    "**dataframe의 데이터 type**\n",
    "- NullType\n",
    "- StringType\n",
    "- BinaryType\n",
    "- BooleanType\n",
    "- DateType\n",
    "- TimestampType\n",
    "- DoubleType\n",
    "- DecimalType\n",
    "- ShortType\n",
    "- ArrayType\n",
    "- MapType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DataFrame 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 list에서 DataFrame 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','Park TH', 2211),\n",
    "        ('1','Lee EB', 7747),\n",
    "        ('2','Kim YW', 2049),\n",
    "        ('2','Hong GD', 1234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| _1|     _2|  _3|\n",
      "+---+-------+----+\n",
      "|  1|Park TH|2211|\n",
      "|  1| Lee EB|7747|\n",
      "|  2| Kim YW|2049|\n",
      "|  2|Hong GD|1234|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**column명 있이 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF1 = spark.createDataFrame(myList,['year','name','phone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|year|   name|phone|\n",
      "+----+-------+-----+\n",
      "|   1|Park TH| 2211|\n",
      "|   1| Lee EB| 7747|\n",
      "|   2| Kim YW| 2049|\n",
      "|   2|Hong GD| 1234|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Row 객체 사용해서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name','phone')\n",
    "row1 = Person('1','Park TH',2211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "         Person('1','Lee EB',7747),\n",
    "         Person('2','Kim YW',2049),\n",
    "         Person('2','Hong GD',1234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF2 = spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|year|   name|phone|\n",
      "+----+-------+-----+\n",
      "|   1|Park TH| 2211|\n",
      "|   1| Lee EB| 7747|\n",
      "|   2| Kim YW| 2049|\n",
      "|   2|Hong GD| 1234|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 RDD에서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','Park TH', 2211),\n",
    "        ('1','Lee EB', 7747),\n",
    "        ('2','Kim YW', 2049),\n",
    "        ('2','Hong GD', 1234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Park TH', 2211),\n",
       " ('1', 'Lee EB', 7747),\n",
       " ('2', 'Kim YW', 2049),\n",
       " ('2', 'Hong GD', 1234)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toDF() 으로 DataFrame 생성\n",
    "myDF3 = myRDD.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| _1|     _2|  _3|\n",
      "+---+-------+----+\n",
      "|  1|Park TH|2211|\n",
      "|  1| Lee EB|7747|\n",
      "|  2| Kim YW|2049|\n",
      "|  2|Hong GD|1234|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createDataFrame() 으로 DataFrame 생성\n",
    "myDF4 = spark.createDataFrame(myRDD,['year','name','phone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|year|   name|phone|\n",
      "+----+-------+-----+\n",
      "|   1|Park TH| 2211|\n",
      "|   1| Lee EB| 7747|\n",
      "|   2| Kim YW| 2049|\n",
      "|   2|Hong GD| 1234|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.1 schema 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType,IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [\n",
    "    (1, 180.3, 79.5),\n",
    "    (2, 190.0, 85.5),\n",
    "    (3, 189.5, 87.5),\n",
    "    (4, 195.0, 90.0),\n",
    "    (5, 200.3, 102.5)\n",
    "]\n",
    "myRDD = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF5 = spark.createDataFrame(myRDD,mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|weight|height|\n",
      "+---+------+------+\n",
      "|  1| 180.3|  79.5|\n",
      "|  2| 190.0|  85.5|\n",
      "|  3| 189.5|  87.5|\n",
      "|  4| 195.0|  90.0|\n",
      "|  5| 200.3| 102.5|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 CSV에서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/CSV2DF.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/CSV2DF.csv\n",
    "0, 0\n",
    "1, 1\n",
    "2, 2\n",
    "3, 3\n",
    "4, 4\n",
    "5, 5\n",
    "6, 6\n",
    "7, 7\n",
    "8, 8\n",
    "9, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4.1 format load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF6 = spark\\\n",
    "    .read\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .options(header='false', inferschema='true', delimiter=',')\\\n",
    "    .load(os.path.join('data/CSV2DF.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|_c0|_c1|\n",
      "+---+---+\n",
      "|  0|0.0|\n",
      "|  1|1.0|\n",
      "|  2|2.0|\n",
      "|  3|3.0|\n",
      "|  4|4.0|\n",
      "|  5|5.0|\n",
      "|  6|6.0|\n",
      "|  7|7.0|\n",
      "|  8|8.0|\n",
      "|  9|9.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4.2 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF7 = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter=',')\\\n",
    "    .csv(os.path.join('data/CSV2DF.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|_c0|_c1|\n",
      "+---+---+\n",
      "|  0|0.0|\n",
      "|  1|1.0|\n",
      "|  2|2.0|\n",
      "|  3|3.0|\n",
      "|  4|4.0|\n",
      "|  5|5.0|\n",
      "|  6|6.0|\n",
      "|  7|7.0|\n",
      "|  8|8.0|\n",
      "|  9|9.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF7.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

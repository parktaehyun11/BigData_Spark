{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression(로지스틱 회귀)\n",
    "\n",
    "- 로지스틱 회귀는 발생할 결과 값이 이진인 경우의 분류에 적용합니다.\n",
    "- 결과값이 이항분포이므로 GLM(Generalized Linear Model)의 한 종류이고, 이 경우 Logit 함수를 link function이라고 합니다.\n",
    "- 로지스틱 회귀식은 입력 값을 받아서 0 ~ 1 사이의 확률을 반환합니다.\n",
    "    - sigmoid 함수를 사용해서 0 ~ 1 사이의 확률 반화\n",
    "- ex) \n",
    "    - 환자의 데이터로부터 병 유무\n",
    "    - 환자의 데이터로부터 사망 또는 생존\n",
    "    - 이메일이 스팸인지 아닌지\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 예제 : 건강 지표를 통해 심장병 유무 판별하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- age - age in years\n",
    "- sex - (1 = male; 0 = female)\n",
    "- cp - chest pain type\n",
    "- trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "- chol - serum cholestoral in mg/dl\n",
    "- fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "- restecg - resting electrocardiographic results\n",
    "- thalach - maximum heart rate achieved\n",
    "- exang - exercise induced angina (1 = yes; 0 = no)\n",
    "- oldpeak - ST depression induced by exercise relative to rest\n",
    "- slope - the slope of the peak exercise ST segment\n",
    "- ca - number of major vessels (0-3) colored by flourosopy\n",
    "- thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "- target - have disease or not (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일 불러오기\n",
    "trainDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true')\\\n",
    "    .load(os.path.join(\"../data/kaggle/heartattack\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형 변환 해주고, 건강 지표와 필요 없는 column drop\n",
    "df = trainDf.withColumn(\"target1\",trainDf['target'].cast(\"double\")).drop('target') #target column double형으로 변경\n",
    "df = df.drop('cp','thal','slope') # 건강지표와 필요 없는 column drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- target1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터와 test 데이터로 나누기\n",
    "train,test = df.randomSplit([0.7,0.3],seed=11) #데이터 반으로 랜덤하게 train, test 데이터로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('testOrtrain',lit('train')) #train data인것 알려주기 위해서 새로운 column 생성해서 train이라고 알려줌 \n",
    "test = test.withColumn('testOrtrain',lit('test')) #test data인것 알려주기 위해서 새로운 column 생성해서 test라고 알려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|testOrtrain|count|\n",
      "+-----------+-----+\n",
      "|      train|  219|\n",
      "|       test|   84|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (train.select('age','sex','trestbps','chol','fbs','restecg','thalach',\n",
    "                   'exang','oldpeak','ca','target1','testOrtrain')\n",
    "        .union(test.select('age','sex','trestbps','chol','fbs','restecg','thalach',\n",
    "                           'exang','oldpeak','ca','target1','testOrtrain'))\n",
    "     )\n",
    "df.groupBy('testOrtrain').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----+---+-------+-------+-----+-------+---+-------+-----------+\n",
      "|age|sex|trestbps|chol|fbs|restecg|thalach|exang|oldpeak| ca|target1|testOrtrain|\n",
      "+---+---+--------+----+---+-------+-------+-----+-------+---+-------+-----------+\n",
      "| 29|  1|     130| 204|  0|      0|    202|    0|    0.0|  0|    1.0|      train|\n",
      "| 34|  0|     118| 210|  0|      1|    192|    0|    0.7|  0|    1.0|      train|\n",
      "| 34|  1|     118| 182|  0|      0|    174|    0|    0.0|  0|    1.0|      train|\n",
      "| 35|  0|     138| 183|  0|      1|    182|    0|    1.4|  0|    1.0|      train|\n",
      "| 35|  1|     120| 198|  0|      1|    130|    1|    1.6|  0|    0.0|      train|\n",
      "+---+---+--------+----+---+-------+-------+-----+-------+---+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"age\",\"sex\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\\\n",
    "                                \"exang\",\"oldpeak\",\"ca\"], outputCol = \"features\") # feature 벡터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[va]) #파이프라인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df) #fit 이용해서 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----+---+-------+-------+-----+-------+---+-------+-----------+--------------------+\n",
      "|age|sex|trestbps|chol|fbs|restecg|thalach|exang|oldpeak| ca|target1|testOrtrain|            features|\n",
      "+---+---+--------+----+---+-------+-------+-----+-------+---+-------+-----------+--------------------+\n",
      "| 29|  1|     130| 204|  0|      0|    202|    0|    0.0|  0|    1.0|      train|(10,[0,1,2,3,6],[...|\n",
      "| 34|  0|     118| 210|  0|      1|    192|    0|    0.7|  0|    1.0|      train|[34.0,0.0,118.0,2...|\n",
      "| 34|  1|     118| 182|  0|      0|    174|    0|    0.0|  0|    1.0|      train|(10,[0,1,2,3,6],[...|\n",
      "| 35|  0|     138| 183|  0|      1|    182|    0|    1.4|  0|    1.0|      train|[35.0,0.0,138.0,1...|\n",
      "| 35|  1|     120| 198|  0|      1|    130|    1|    1.6|  0|    0.0|      train|[35.0,1.0,120.0,1...|\n",
      "+---+---+--------+----+---+-------+-------+-----+-------+---+-------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=myDf.filter(myDf['testOrtrain']=='train')\n",
    "trainDf,validateDf = train.randomSplit([0.7,0.3],seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf=myDf.filter(myDf['testOrtrain']=='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logisticRegression 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = (LogisticRegression().\n",
    "    setLabelCol('target1').\n",
    "    setFeaturesCol('features').\n",
    "    setRegParam(0.0).\n",
    "    setMaxIter(100).\n",
    "    setElasticNetParam(0.)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel=lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측\n",
    "- transform() 함수에 데이터 넣어주면 예측값 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrDf = lrModel.transform(validateDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n",
      "|prediction|target1|count|\n",
      "+----------+-------+-----+\n",
      "|       1.0|    1.0|   31|\n",
      "|       0.0|    1.0|    4|\n",
      "|       1.0|    0.0|   11|\n",
      "|       0.0|    0.0|   21|\n",
      "+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrDf.groupBy('prediction','target1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.09821428571428 %\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction',labelCol='target1')\n",
    "print(evaluator.evaluate(lrDf)*100 ,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Titanic\n",
    "- 타아타닉 사고에서 2224명의 승객 중 1502명이 사망했습니다.\n",
    "- 사고 당시의 탑승객 위치, 상황 등을 분석하여 생존 여부를 예측 해 보도록 하겠습니다.\n",
    "- 생존 했는지 안했는지 이진분류이기 때문에 logistic regression을 적용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.1 Train 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.csv 파일 불러오기\n",
    "trainDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true')\\\n",
    "    .load(os.path.join(\"../data/kaggle/titanic/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.2 Test 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.csv 파일 불러오기\n",
    "testDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true')\\\n",
    "    .load(os.path.join(\"../data/kaggle/titanic/test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|330911|7.8292| null|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0|363272|   7.0| null|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0|240276|9.6875| null|       Q|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train, test 데이터 하나로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 testOrtrain 추가해서 train 데이터인지 test 데이터인지 구별해 주기\n",
    "trainDf = trainDf.withColumn('testOrtrain',lit('train'))\n",
    "testDf = testDf.withColumn('testOrtrain',lit('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testDf 에는 Survived 없으므로 임의로 99 추가해주기\n",
    "testDf = testDf.withColumn('Survived',lit(99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`union`** 기능을 이용해서 trainDf 와 testDf를 하나로 합쳐주도록 하겠습니다.  \n",
    "두 DataFrame의 column 수와 데이터type이 일치해야 union 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainDf.select('PassengerId','Survived','Pclass','Name','Sex','Age',\\\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked','testOrtrain')\\\n",
    "            .union(testDf.select('PassengerId','Survived','Pclass','Name','Sex','Age',\\\n",
    "                   'SibSp','Parch','Ticket','Fare','Cabin','Embarked','testOrtrain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- testOrtrain: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 합친 데이터에서 train 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|testOrtrain|Survived|                Name|\n",
      "+-----------+--------+--------------------+\n",
      "|      train|       0|Braund, Mr. Owen ...|\n",
      "|      train|       1|Cumings, Mrs. Joh...|\n",
      "|      train|       1|Heikkinen, Miss. ...|\n",
      "|      train|       1|Futrelle, Mrs. Ja...|\n",
      "|      train|       0|Allen, Mr. Willia...|\n",
      "|      train|       0|    Moran, Mr. James|\n",
      "|      train|       0|McCarthy, Mr. Tim...|\n",
      "|      train|       0|Palsson, Master. ...|\n",
      "|      train|       1|Johnson, Mrs. Osc...|\n",
      "|      train|       1|Nasser, Mrs. Nich...|\n",
      "+-----------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('testOrtrain','Survived','Name')\\\n",
    "    .filter(df['testOrtrain']=='train').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 합친 데이터에서 test 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|testOrtrain|Survived|                Name|\n",
      "+-----------+--------+--------------------+\n",
      "|       test|      99|    Kelly, Mr. James|\n",
      "|       test|      99|Wilkes, Mrs. Jame...|\n",
      "|       test|      99|Myles, Mr. Thomas...|\n",
      "|       test|      99|    Wirz, Mr. Albert|\n",
      "|       test|      99|Hirvonen, Mrs. Al...|\n",
      "|       test|      99|Svensson, Mr. Joh...|\n",
      "|       test|      99|Connolly, Miss. Kate|\n",
      "|       test|      99|Caldwell, Mr. Alb...|\n",
      "|       test|      99|Abrahim, Mrs. Jos...|\n",
      "|       test|      99|Davies, Mr. John ...|\n",
      "+-----------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('testOrtrain','Survived','Name')\\\n",
    "    .filter(df['testOrtrain']=='test').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 결측치 처리하기\n",
    "- 결측값들은 보통 결측으로 제외하거나 평균으로 대체 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "|PassengerId|Survived|Pclass|Name| Sex| Age|SibSp|Parch|Ticket|Fare|Embarked|testOrtrain|\n",
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "|       1309|    1309|  1309|1309|1309|1046| 1309| 1309|  1309|1308|    1307|       1309|\n",
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.agg(*[count(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 263, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 1, 'Embarked': 2, 'testOrtrain': 0}\n"
     ]
    }
   ],
   "source": [
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {c: countNull(df,c) for c in ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Embarked','testOrtrain']}\n",
    "print (missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgAge = df.agg(F.avg(df['Age']).alias('meanAge')).collect()\n",
    "avgFare = df.agg(F.avg(df['Fare']).alias('meanFare')).collect()\n",
    "#avgEmbarked = df.agg(F.avg(df['Embarked']).alias('meanEmbarked')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,isnull\n",
    "df=df.withColumn(\"Age\", when(isnull(df['Age']), avgAge[0]['meanAge']).otherwise(df.Age))\n",
    "df=df.withColumn(\"Fare\", when(isnull(df['Fare']), avgFare[0]['meanFare']).otherwise(df.Fare))\n",
    "#df=df.withColumn(\"Embarked\", when(isnull(df['Embarked']), avgEmbarked[0]['meanEmbarked']).otherwise(df.Embarked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "|PassengerId|Survived|Pclass|Name| Sex| Age|SibSp|Parch|Ticket|Fare|Embarked|testOrtrain|\n",
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "|       1309|    1309|  1309|1309|1309|1309| 1309| 1309|  1309|1309|    1307|       1309|\n",
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.agg(*[count(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 0, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'Embarked': 2, 'testOrtrain': 0}\n"
     ]
    }
   ],
   "source": [
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {c: countNull(df,c) for c in ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Embarked','testOrtrain']}\n",
    "print (missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked 결측치 있는 행 제거\n",
    "df = df.na.drop('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 0, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'Embarked': 0, 'testOrtrain': 0}\n"
     ]
    }
   ],
   "source": [
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {c: countNull(df,c) for c in ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Embarked','testOrtrain']}\n",
    "print (missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "|PassengerId|Survived|Pclass|Name| Sex| Age|SibSp|Parch|Ticket|Fare|Embarked|testOrtrain|\n",
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "|       1307|    1307|  1307|1307|1307|1307| 1307| 1307|  1307|1307|    1307|       1307|\n",
      "+-----------+--------+------+----+----+----+-----+-----+------+----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.agg(*[count(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 label, features 구성\n",
    "- 현재 Survived column이 integer로 정의되어 있으니 double 형으로 우선 바꿔 주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.withColumn(\"Survive\",trainDf['Survived']\n",
    "     .cast(\"double\"))\n",
    "      .drop(\"Survived\")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Survive|count|\n",
      "+-------+-----+\n",
      "|    0.0|  549|\n",
      "|    1.0|  340|\n",
      "|   99.0|  418|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Survive').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string인 `Sex`와 `Embarked`를 StringIndexer로 변환해 주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "SexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "EmbarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 `feature` column을 생성해 주도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- testOrtrain: string (nullable = false)\n",
      " |-- Survive: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"Pclass\",\"SexIndex\",\"Age\",\"SibSp\",\"Parch\",\\\n",
    "                                \"Fare\",\"EmbarkedIndex\"],\\\n",
    "                     outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline`으로 구성해서 `fit()`을 실행하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[SexIndexer,EmbarkedIndexer,va])\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1307"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf = model.transform(df)\n",
    "myDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|Survive|            features|\n",
      "+-------+--------------------+\n",
      "|    0.0|[3.0,0.0,22.0,1.0...|\n",
      "|    1.0|[1.0,1.0,38.0,1.0...|\n",
      "|    1.0|[3.0,1.0,26.0,0.0...|\n",
      "|    1.0|[1.0,1.0,35.0,1.0...|\n",
      "|    0.0|(7,[0,2,5],[3.0,3...|\n",
      "|    0.0|[3.0,0.0,29.88113...|\n",
      "|    0.0|(7,[0,2,5],[1.0,5...|\n",
      "|    0.0|[3.0,0.0,2.0,3.0,...|\n",
      "|    1.0|[3.0,1.0,27.0,0.0...|\n",
      "|    1.0|[2.0,1.0,14.0,1.0...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('Survive','features').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "trainingDf = myDf.filter(myDf['testOrtrain']=='train')\n",
    "testDf = myDf.filter(myDf['testOrtrain']=='test')\n",
    "print(trainDf.count())\n",
    "print(testDf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf,validateDf = trainingDf.randomSplit([0.7,0.3],seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "print(trainDf.count())\n",
    "print(validateDf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- testOrtrain: string (nullable = false)\n",
      " |-- Survive: double (nullable = true)\n",
      " |-- SexIndex: double (nullable = false)\n",
      " |-- EmbarkedIndex: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- testOrtrain: string (nullable = false)\n",
      " |-- Survive: double (nullable = true)\n",
      " |-- SexIndex: double (nullable = false)\n",
      " |-- EmbarkedIndex: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validateDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 LogisticRegression 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Survive|count|\n",
      "+-------+-----+\n",
      "|    0.0|  385|\n",
      "|    1.0|  243|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.groupBy('Survive').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('Survive').\\\n",
    "    setFeaturesCol('features').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel=lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrDf = lrModel.transform(validateDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- testOrtrain: string (nullable = false)\n",
      " |-- Survive: double (nullable = true)\n",
      " |-- SexIndex: double (nullable = false)\n",
      " |-- EmbarkedIndex: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------+\n",
      "|Survive|       rawPrediction|         probability|prediction|\n",
      "+-------+--------------------+--------------------+----------+\n",
      "|    0.0|[1.75048837387783...|[0.85201438959993...|       0.0|\n",
      "|    0.0|[0.68880110544961...|[0.66570017364378...|       0.0|\n",
      "|    1.0|[-1.6072526323220...|[0.16697039891915...|       1.0|\n",
      "|    0.0|[1.85316538795041...|[0.86449832760428...|       0.0|\n",
      "|    0.0|[3.06289988462698...|[0.95533619549127...|       0.0|\n",
      "|    0.0|[-0.9069728065691...|[0.28761969386244...|       1.0|\n",
      "|    0.0|[2.32973077312161...|[0.91130957904571...|       0.0|\n",
      "|    1.0|[1.13927934115816...|[0.75754730060353...|       0.0|\n",
      "|    1.0|[1.27143083992238...|[0.78098758571895...|       0.0|\n",
      "|    0.0|[-0.3564542334155...|[0.41181816621951...|       1.0|\n",
      "|    0.0|[2.17107581221118...|[0.89762187260460...|       0.0|\n",
      "|    1.0|[-0.8142202518985...|[0.30699191545836...|       1.0|\n",
      "|    0.0|[-0.1466323274074...|[0.46340745946973...|       1.0|\n",
      "|    0.0|[1.88524985402134...|[0.86821297261592...|       0.0|\n",
      "|    0.0|[-0.0758965775697...|[0.48103495841273...|       1.0|\n",
      "|    1.0|[-0.7546815966883...|[0.31980206118641...|       1.0|\n",
      "|    1.0|[-0.8142202518985...|[0.30699191545836...|       1.0|\n",
      "|    0.0|[1.88667559661530...|[0.86837601932924...|       0.0|\n",
      "|    1.0|[-1.1515783642342...|[0.24020090647523...|       1.0|\n",
      "|    0.0|[3.38525623710996...|[0.96724056532412...|       0.0|\n",
      "+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrDf.select('Survive','rawPrediction','probability','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.89062107115916 %\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction',labelCol='Survive')\n",
    "print(evaluator.evaluate(lrDf)*100 ,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
